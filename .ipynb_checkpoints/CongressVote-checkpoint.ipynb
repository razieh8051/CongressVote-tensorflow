{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 layered NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, weights, biases):\n",
    "    #Create 2-layered NN\n",
    "    input_layer= tf.matmul(inputs,weights['input'])\n",
    "    # Hidden layer 1 (relu activation)\n",
    "    hidden_layer1= tf.nn.relu(input_layer + biases['input'])\n",
    "    #  Hidden layer 2 (relu activation)\n",
    "    hidden_layer2= tf.nn.relu(tf.matmul(hidden_layer1,weights['hidden1'])+biases['hidden1'])\n",
    "    # Output layer (linear activation)\n",
    "    output_layer= tf.matmul(hidden_layer2, weights['hidden2']) + biases['hidden2']\n",
    "\n",
    "    return output_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Column is the label and others are the features\n",
    "COLUMNS = ['className','handicapped-infants','water-project-cost-sharing','adoption-of-the-budget-resolution',\n",
    "'physician-fee-freeze','el-salvador-aid','religious-groups-in-schools','anti-satellite-test-ban',\n",
    "'aid-to-nicaraguan-contras','mx-missle','immigration','synfuels-corporation-cutback','education-spending',\n",
    "'superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa']\n",
    "\n",
    "# Read data into CSV file and convert ? to NaN\n",
    "df = pd.read_csv(os.path.join(\"./house-votes-84.data.txt\")\n",
    "                       , names=COLUMNS\n",
    "                       , skipinitialspace=True\n",
    "                       , na_values=\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with missing data (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rows with NaN values are removed from data\n",
    "df.replace([\"NaN\"], np.nan, inplace = True)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "  \n",
    "#New row numbers after removing NaN values\n",
    "rows_number=df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label column republican/democrat to 1 and 0\n",
    "for i in range(0,len(COLUMNS)):\n",
    "    cleanup_nums = {COLUMNS[i]: {\"y\":1, \"n\":0, \"republican\":1,\"democrat\":0}}\n",
    "    df.replace(cleanup_nums, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Make a 2 column matrix for the labels column\n",
    "labels=np.zeros((rows_number, 2))\n",
    "label=df['className']\n",
    "for i in range(0,len(label)):\n",
    "    if(label[i]==0):\n",
    "        labels[i,0]=0\n",
    "        labels[i,1]=1\n",
    "    else:\n",
    "        labels[i,0]=1\n",
    "        labels[i,1]=0\n",
    "print (labels)\n",
    "inputs=df[COLUMNS[2:]]\n",
    "inputs=inputs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameters Training\n",
    "learning_rate = 0.5\n",
    "epochs = 50\n",
    "batch_size = 50 #small dataset\n",
    "\n",
    "n_features = 15\n",
    "n_calsses=2\n",
    "\n",
    "#HyperParameters NN\n",
    "hidden_nodes_layer1=10\n",
    "hidden_nodes_layer2=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables for weights and biases\n",
    "input_weights = tf.Variable(tf.random_normal([n_features, hidden_nodes_layer1]))\n",
    "input_biases = tf.Variable(tf.zeros([1,hidden_nodes_layer1]))\n",
    "\n",
    "hidden1_weights = tf.Variable(tf.random_normal([hidden_nodes_layer1, hidden_nodes_layer2]))\n",
    "hidden1_biases = tf.Variable(tf.zeros([1,hidden_nodes_layer2]))\n",
    "\n",
    "hidden2_weights = tf.Variable(tf.random_normal([hidden_nodes_layer2, n_calsses]))\n",
    "hidden2_biases = tf.Variable(tf.zeros([1,n_calsses]))\n",
    "\n",
    "weights={'input':input_weights,'hidden1':hidden1_weights,'hidden2':hidden2_weights}\n",
    "biases={'input':input_biases,'hidden1':hidden1_biases,'hidden2':hidden2_biases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholders to initialize \n",
    "x = tf.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, n_calsses])\n",
    "\n",
    "#Create the NN model\n",
    "prediction=create_model(x,weights,biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, 'cost=', 23.092151482899983)\n",
      "('Epoch:', 2, 'cost=', 14.385650416215261)\n",
      "('Epoch:', 3, 'cost=', 0.5331760247548422)\n",
      "('Epoch:', 4, 'cost=', 0.5298467874526978)\n",
      "('Epoch:', 5, 'cost=', 0.5287722746531168)\n",
      "('Epoch:', 6, 'cost=', 0.5283738970756531)\n",
      "('Epoch:', 7, 'cost=', 0.5281945168972015)\n",
      "('Epoch:', 8, 'cost=', 0.5280855695406597)\n",
      "('Epoch:', 9, 'cost=', 0.5279851059118906)\n",
      "('Epoch:', 10, 'cost=', 0.5278985301653545)\n",
      "('Epoch:', 11, 'cost=', 0.5278183420499166)\n",
      "('Epoch:', 12, 'cost=', 0.5277517636617025)\n",
      "('Epoch:', 13, 'cost=', 0.5276897450288137)\n",
      "('Epoch:', 14, 'cost=', 0.5276283125082651)\n",
      "('Epoch:', 15, 'cost=', 0.5275668303171794)\n",
      "('Epoch:', 16, 'cost=', 0.5275066196918488)\n",
      "('Epoch:', 17, 'cost=', 0.5274458924929302)\n",
      "('Epoch:', 18, 'cost=', 0.5273832082748413)\n",
      "('Epoch:', 19, 'cost=', 0.5273198187351227)\n",
      "('Epoch:', 20, 'cost=', 0.5272558331489563)\n",
      "('Epoch:', 21, 'cost=', 0.5271909137566885)\n",
      "('Epoch:', 22, 'cost=', 0.5271251400311787)\n",
      "('Epoch:', 23, 'cost=', 0.527058204015096)\n",
      "('Epoch:', 24, 'cost=', 0.5269859135150909)\n",
      "('Epoch:', 25, 'cost=', 0.5269113679726918)\n",
      "('Epoch:', 26, 'cost=', 0.5268356700738271)\n",
      "('Epoch:', 27, 'cost=', 0.5267588992913564)\n",
      "('Epoch:', 28, 'cost=', 0.52668097615242)\n",
      "('Epoch:', 29, 'cost=', 0.5266018807888031)\n",
      "('Epoch:', 30, 'cost=', 0.5265219807624817)\n",
      "('Epoch:', 31, 'cost=', 0.5264430642127991)\n",
      "('Epoch:', 32, 'cost=', 0.5262985328833262)\n",
      "('Epoch:', 33, 'cost=', 0.5260780056317647)\n",
      "('Epoch:', 34, 'cost=', 0.5258534550666809)\n",
      "('Epoch:', 35, 'cost=', 0.5256303946177165)\n",
      "('Epoch:', 36, 'cost=', 0.5253984332084656)\n",
      "('Epoch:', 37, 'cost=', 0.52517103155454)\n",
      "('Epoch:', 38, 'cost=', 0.5380191306273142)\n",
      "('Epoch:', 39, 'cost=', 0.98368901014328)\n",
      "('Epoch:', 40, 'cost=', 0.5285544196764628)\n",
      "('Epoch:', 41, 'cost=', 0.4863718847433727)\n",
      "('Epoch:', 42, 'cost=', 0.42703688144683843)\n",
      "('Epoch:', 43, 'cost=', 0.36916816234588623)\n",
      "('Epoch:', 44, 'cost=', 0.3231025437513987)\n",
      "('Epoch:', 45, 'cost=', 0.2864896208047867)\n",
      "('Epoch:', 46, 'cost=', 0.2968011697133382)\n",
      "('Epoch:', 47, 'cost=', 0.2589435329039892)\n",
      "('Epoch:', 48, 'cost=', 0.21183527012666065)\n",
      "('Epoch:', 49, 'cost=', 0.17815042535463968)\n",
      "('Epoch:', 50, 'cost=', 0.15924159189065298)\n",
      "Accuracy: 0.9137931\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "    #Training cycle\n",
    "    # Loop epochs\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0       \n",
    "        total_batch = int(len(x_train)/batch_size)\n",
    "        X_batches = np.array_split(x_train, total_batch)\n",
    "        Y_batches = np.array_split(y_train, total_batch)\n",
    "        # Loop batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # Run optimization and loss \n",
    "            _, cost = sess.run([optimizer, loss_function], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # average cost\n",
    "            avg_cost += cost / total_batch\n",
    "        print(\"Epoch:\", epoch+1, \"cost=\", avg_cost)\n",
    "    \n",
    "    #accuracy\n",
    "    print(\"Accuracy: \" + str(accuracy.eval(feed_dict={x: x_test,\n",
    "                                                          y: y_test})))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
