{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(inputs, weights, biases):\n",
    "    #Create 2-layered NN\n",
    "    input_layer= tf.matmul(inputs,weights['input'])\n",
    "    # Hidden layer 1 (relu activation)\n",
    "    hidden_layer1= tf.nn.relu(input_layer + biases['input'])\n",
    "    #  Hidden layer 2 (relu activation)\n",
    "    hidden_layer2= tf.nn.relu(tf.matmul(hidden_layer1,weights['hidden1'])+biases['hidden1'])\n",
    "    # Output layer (linear activation)\n",
    "    output_layer= tf.matmul(hidden_layer2, weights['hidden2']) + biases['hidden2']\n",
    "\n",
    "    return output_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Column is the label and others are the features\n",
    "COLUMNS = ['className','handicapped-infants','water-project-cost-sharing','adoption-of-the-budget-resolution',\n",
    "'physician-fee-freeze','el-salvador-aid','religious-groups-in-schools','anti-satellite-test-ban',\n",
    "'aid-to-nicaraguan-contras','mx-missle','immigration','synfuels-corporation-cutback','education-spending',\n",
    "'superfund-right-to-sue','crime','duty-free-exports','export-administration-act-south-africa']\n",
    "\n",
    "# Read data into CSV file and convert ? to NaN\n",
    "df = pd.read_csv(os.path.join(\"./house-votes-84.data.txt\")\n",
    "                       , names=COLUMNS\n",
    "                       , skipinitialspace=True\n",
    "                       , na_values=\"?\")\n",
    "\n",
    "#Rows with NaN values are removed from data\n",
    "df.replace([\"NaN\"], np.nan, inplace = True)\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)\n",
    "  \n",
    "#New row numbers after removing NaN values\n",
    "rows_number=df.shape[0]\n",
    "\n",
    "\n",
    "#Label column republican/democrat to 1 and 0\n",
    "for i in range(0,len(COLUMNS)):\n",
    "    cleanup_nums = {COLUMNS[i]: {\"y\":1, \"n\":0, \"republican\":1,\"democrat\":0}}\n",
    "    df.replace(cleanup_nums, inplace=True)\n",
    "\n",
    "#Make a 2 column matrix for the labels column\n",
    "labels=np.zeros((rows_number, 2))\n",
    "label=df['className']\n",
    "for i in range(0,len(label)):\n",
    "    if(label[i]==0):\n",
    "        labels[i,0]=0\n",
    "        labels[i,1]=1\n",
    "    else:\n",
    "        labels[i,0]=1\n",
    "        labels[i,1]=0\n",
    "\n",
    "inputs=df[COLUMNS[2:]]\n",
    "inputs=inputs.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameters Training\n",
    "learning_rate = 0.5\n",
    "epochs = 50\n",
    "batch_size = 50 #small dataset\n",
    "\n",
    "n_features = 15\n",
    "n_calsses=2\n",
    "\n",
    "#HyperParameters NN\n",
    "hidden_nodes_layer1=10\n",
    "hidden_nodes_layer2=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define weights and biases\n",
    "input_weights = tf.Variable(tf.random_normal([n_features, hidden_nodes_layer1]))\n",
    "input_biases = tf.Variable(tf.zeros([1,hidden_nodes_layer1]))\n",
    "\n",
    "hidden1_weights = tf.Variable(tf.random_normal([hidden_nodes_layer1, hidden_nodes_layer2]))\n",
    "hidden1_biases = tf.Variable(tf.zeros([1,hidden_nodes_layer2]))\n",
    "\n",
    "hidden2_weights = tf.Variable(tf.random_normal([hidden_nodes_layer2, n_calsses]))\n",
    "hidden2_biases = tf.Variable(tf.zeros([1,n_calsses]))\n",
    "\n",
    "weights={'input':input_weights,'hidden1':hidden1_weights,'hidden2':hidden2_weights}\n",
    "biases={'input':input_biases,'hidden1':hidden1_biases,'hidden2':hidden2_biases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, n_calsses])\n",
    "\n",
    "#Create the NN model\n",
    "prediction=create_model(x,weights,biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, 'cost=', 11.934635957082113)\n",
      "('Epoch:', 2, 'cost=', 9.164107183615366)\n",
      "('Epoch:', 3, 'cost=', 0.7031195958455404)\n",
      "('Epoch:', 4, 'cost=', 0.6862129370371501)\n",
      "('Epoch:', 5, 'cost=', 0.6808030009269714)\n",
      "('Epoch:', 6, 'cost=', 0.6755284269650778)\n",
      "('Epoch:', 7, 'cost=', 0.6692893505096436)\n",
      "('Epoch:', 8, 'cost=', 0.659934937953949)\n",
      "('Epoch:', 9, 'cost=', 0.6465867956479391)\n",
      "('Epoch:', 10, 'cost=', 0.6186259190241495)\n",
      "('Epoch:', 11, 'cost=', 0.5817812085151672)\n",
      "('Epoch:', 12, 'cost=', 0.5476115544637045)\n",
      "('Epoch:', 13, 'cost=', 0.5193539460500082)\n",
      "('Epoch:', 14, 'cost=', 0.49651218454043067)\n",
      "('Epoch:', 15, 'cost=', 0.4721291561921438)\n",
      "('Epoch:', 16, 'cost=', 0.4525176088015238)\n",
      "('Epoch:', 17, 'cost=', 0.4333740969498952)\n",
      "('Epoch:', 18, 'cost=', 0.4179316560427348)\n",
      "('Epoch:', 19, 'cost=', 0.40368167559305823)\n",
      "('Epoch:', 20, 'cost=', 0.38986657063166297)\n",
      "('Epoch:', 21, 'cost=', 0.37580686807632446)\n",
      "('Epoch:', 22, 'cost=', 0.3619091808795929)\n",
      "('Epoch:', 23, 'cost=', 0.34951096773147583)\n",
      "('Epoch:', 24, 'cost=', 0.33860865235328674)\n",
      "('Epoch:', 25, 'cost=', 0.3303271333376567)\n",
      "('Epoch:', 26, 'cost=', 0.32174429297447205)\n",
      "('Epoch:', 27, 'cost=', 0.3130918641885122)\n",
      "('Epoch:', 28, 'cost=', 0.3054642627636592)\n",
      "('Epoch:', 29, 'cost=', 0.2986808468898137)\n",
      "('Epoch:', 30, 'cost=', 0.29209964474042255)\n",
      "('Epoch:', 31, 'cost=', 0.28577104210853577)\n",
      "('Epoch:', 32, 'cost=', 0.27801532049973804)\n",
      "('Epoch:', 33, 'cost=', 0.2703862190246582)\n",
      "('Epoch:', 34, 'cost=', 0.2596702923377355)\n",
      "('Epoch:', 35, 'cost=', 0.24802370369434357)\n",
      "('Epoch:', 36, 'cost=', 0.23730463286240894)\n",
      "('Epoch:', 37, 'cost=', 0.2241715838511785)\n",
      "('Epoch:', 38, 'cost=', 0.21234445770581567)\n",
      "('Epoch:', 39, 'cost=', 0.20231347282727558)\n",
      "('Epoch:', 40, 'cost=', 0.19343241552511853)\n",
      "('Epoch:', 41, 'cost=', 0.1844775229692459)\n",
      "('Epoch:', 42, 'cost=', 0.17467190821965534)\n",
      "('Epoch:', 43, 'cost=', 0.16622452189524967)\n",
      "('Epoch:', 44, 'cost=', 0.15884636094172797)\n",
      "('Epoch:', 45, 'cost=', 0.15249203890562057)\n",
      "('Epoch:', 46, 'cost=', 0.1468878984451294)\n",
      "('Epoch:', 47, 'cost=', 0.14202628781398138)\n",
      "('Epoch:', 48, 'cost=', 0.13539429754018784)\n",
      "('Epoch:', 49, 'cost=', 0.12066074460744858)\n",
      "('Epoch:', 50, 'cost=', 0.11062228182951611)\n",
      "Accuracy: 0.9137931\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    \n",
    "    #Training cycle\n",
    "    # Loop epochs\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0       \n",
    "        total_batch = int(len(x_train)/batch_size)\n",
    "        X_batches = np.array_split(x_train, total_batch)\n",
    "        Y_batches = np.array_split(y_train, total_batch)\n",
    "        # Loop batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # Run optimization and loss \n",
    "            _, cost = sess.run([optimizer, loss_function], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # average cost\n",
    "            avg_cost += cost / total_batch\n",
    "        print(\"Epoch:\", epoch+1, \"cost=\", avg_cost)\n",
    "    \n",
    "    #accuracy\n",
    "    print(\"Accuracy: \" + str(accuracy.eval(feed_dict={x: x_test,\n",
    "                                                          y: y_test})))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
